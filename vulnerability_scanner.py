#!/usr/bin/env python3

import json
import gzip
import time
import sys
import os
from pathlib import Path

from rdflib import Graph, Literal, RDF, URIRef, Namespace
from rdflib.namespace import XSD

from sentence_transformers import SentenceTransformer, util

# --- Configuration ---
RESULTS_JSON = os.getenv("RESULTS_JSON", "results-scanner-modelscan.json")
OUTPUT_TTL = os.getenv("LOCAL_TTL", "local_ontology.ttl")

# Ontology namespace
ONTOSEC = Namespace("http://example.org/ontosec#")

# Embedding model
MODEL_NAME = "all-mpnet-base-v2"

# CVE folder
cve_folder = Path(os.getenv("CVE_FEEDS_PATH", "CVE-feeds/"))
cve_list = []

# --- Load CVEs from local GZ files ---
for gz_file in cve_folder.glob("nvdcve-2.0-*.json.gz"):
    print("Loading:", gz_file)
    with gzip.open(gz_file, "rt", encoding="utf-8") as f:
        data = json.load(f)
        for item in data.get("vulnerabilities", []):
            try:
                cve = item.get("cve", {})
                cve_id = cve.get("id")
                desc = cve.get("descriptions", [{}])[0].get("value")
                if cve_id and desc:
                    cve_list.append((cve_id, desc))
            except Exception:
                continue

print(f"Loaded {len(cve_list)} CVEs from local feeds.")

if not cve_list:
    print("ERROR: No CVEs loaded! Check folder path and file names.")
    sys.exit(1)

# --- Pre-filter CVEs by keywords ---
KEYWORDS = ["python", "pickle", "pytorch"]
filtered_cves = [
    (cve_id, desc) for cve_id, desc in cve_list
    if any(k in desc.lower() for k in KEYWORDS)
]
print(f"Filtered CVEs for keywords {KEYWORDS}: {len(filtered_cves)}")

if not filtered_cves:
    print("WARNING: No CVEs match the keywords. Using all CVEs.")
    filtered_cves = cve_list

# --- Load scan results ---
results_path = Path(RESULTS_JSON)
if not results_path.exists():
    print(f"ERROR: {RESULTS_JSON} not found", file=sys.stderr)
    sys.exit(1)

with open(results_path, "r") as f:
    data = json.load(f)

issues = data.get("issues", [])
input_path = data.get("summary", {}).get("input_path", "").lower()

if not issues:
    print("No issues found in results — nothing to enrich.")
    sys.exit(0)

# --- Detect model framework automatically ---
context_keywords = ""
if input_path.endswith(".pt") or "torch" in input_path:
    context_keywords = "PyTorch machine learning model unsafe pickle deserialization "
elif input_path.endswith(".h5") or "keras" in input_path:
    context_keywords = "Keras TensorFlow model unsafe serialization "
elif input_path.endswith(".onnx"):
    context_keywords = "ONNX model serialization security "
elif input_path.endswith(".pkl") or "pickle" in input_path:
    context_keywords = "Python pickle deserialization vulnerability "
else:
    context_keywords = "machine learning model serialization "

print(f"[*] Auto-detected context keywords: {context_keywords.strip()}")

# --- Load embedding model ---
print(f"Loading embedding model `{MODEL_NAME}` …")
model = SentenceTransformer(MODEL_NAME)

# --- Compute embeddings for CVE descriptions (batching) ---
cve_texts = [desc for (_id, desc) in filtered_cves]
print("Encoding CVE descriptions …")
cve_embeddings = model.encode(cve_texts, batch_size=64, convert_to_tensor=True)

# --- Build RDF graph ---
g = Graph()
g.bind("ontosec", ONTOSEC)

issue_counter = 1
for issue in issues:
    desc = issue.get("description", "")
    module = issue.get("module", "")
    operator = issue.get("operator", "")
    source = issue.get("source", "")
    severity = issue.get("severity", "")
    
    enriched_desc = (context_keywords + desc).strip()
    issue_uri = URIRef(ONTOSEC[f"Issue_{issue_counter}"])
    
    # Add base ontology triples
    g.add((issue_uri, RDF.type, ONTOSEC.Issue))
    g.add((issue_uri, ONTOSEC.issueID, Literal(issue_counter, datatype=XSD.int)))
    g.add((issue_uri, ONTOSEC.Module, Literal(module, datatype=XSD.string)))
    g.add((issue_uri, ONTOSEC.Operator, Literal(operator, datatype=XSD.string)))
    g.add((issue_uri, ONTOSEC.Source, Literal(source, datatype=XSD.string)))
    g.add((issue_uri, ONTOSEC.Detail, Literal(enriched_desc, datatype=XSD.string)))
    
    # --- Semantic match to CVE ---
    print(f"\nIssue #{issue_counter}: {enriched_desc}")
    issue_emb = model.encode(enriched_desc, convert_to_tensor=True)
    cos_scores = util.cos_sim(issue_emb, cve_embeddings)[0]
    best_idx = int(cos_scores.argmax())
    best_score = float(cos_scores[best_idx])
    best_cve_id, best_cve_desc = filtered_cves[best_idx]
    
    print(f" → Best CVE match: {best_cve_id} (score {best_score:.3f})")
    print(f"    CVE description: {best_cve_desc}")
    
    # Add to RDF if score is above threshold
    THRESH = 0.50
    if best_score >= THRESH:
        g.add((issue_uri, ONTOSEC.hasCVE, Literal(best_cve_id, datatype=XSD.string)))
        g.add((issue_uri, ONTOSEC.hasCVEdescription, Literal(best_cve_desc, datatype=XSD.string)))
    else:
        print("    No strong CVE match found (score below threshold)")
    
    issue_counter += 1
    time.sleep(0.05)

# --- Load existing TTL and merge with new data ---
print(f"\nLoading existing TTL from {OUTPUT_TTL}")
existing_g = Graph()
try:
    existing_g.parse(OUTPUT_TTL, format="turtle")
    print(f"Loaded {len(existing_g)} existing triples")
except FileNotFoundError:
    print("No existing TTL file found, will create new one")

# Merge existing and new triples
merged_g = existing_g + g

# --- Serialize to TTL ---
print(f"\nSerializing merged TTL to {OUTPUT_TTL}")
merged_g.serialize(destination=OUTPUT_TTL, format="turtle")
print(f"Done. Enriched ontology file saved with {len(merged_g)} total triples.")
